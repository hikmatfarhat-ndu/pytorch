{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGNzREfyiWT0TsXaEghBPU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/pytorch/blob/main/dl_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Learning\n",
        "\n",
        "Machine learning tasks can be loosely grouped into three categories\n",
        "1. Supervised Learning\n",
        "1. Unsupervised Learning\n",
        "1. Reinforcement Learning\n",
        "\n",
        "In this workshop se we will practice mostly **supervised learning**. In supervised learning we are given N data points.\n",
        "\n",
        "$$Data=\\{(x_1,y_1),\\ldots,(x_N,y_N)\\}$$ \n",
        "\n",
        "presumably generated (or sampled) by some __unknown__  function __y=f(x)__. Our goal basically is to __learn__ (an approximation of) __f(x)__. If we do it successfully then for any input __x__ we can compute __y=f(x)__. \n",
        "We will see two  types of supervised learning  __classification__ and **regression**. In the first case  __y__ belongs to a discrete set of classes _C_. \n",
        "In this notebook we give the first example of **supervised learning**. We are given a set of (image,label) pairs (CIFAR10)   where each image can be in one of the __ten__ classes: ship, horse, car...etc and so each label is a value between 0 and 9 denoting the class of image. For example an image with an associated label of 8 is that of a ship. \n",
        "\n",
        "To simplify matters we will group all \"machines\" (ship, car,...) into one group and all living things (horse, dog,...) into another\n"
      ],
      "metadata": {
        "id": "u1vtIxur97UM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4j6mq7QsZzsF"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision as vision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_train=vision.datasets.CIFAR10(\".\",download=True)\n",
        "cifar10_test=vision.datasets.CIFAR10(\".\",download=True,train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXoGGW1OaAEZ",
        "outputId": "a7c454c2-cc5a-4c8d-d5bd-4ae620b1f4a5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we create torch **tensors** from the datasets. For now, we think of a torch **tensor** as a multidimensional array.\n",
        "\n",
        "**Note**: the pixel values are divided by the maximal value (255). This is ofen the case to aid with the convergence"
      ],
      "metadata": {
        "id": "pI3ML83Va19K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_train=torch.tensor(cifar10_train.data,dtype=torch.float32)/255.\n",
        "img_test=torch.tensor(cifar10_test.data,dtype=torch.float32)/255.\n",
        "label_train=torch.tensor(cifar10_train.targets,dtype=torch.float32)\n",
        "label_test=torch.tensor(cifar10_test.targets,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "7rOOuE5ra-qC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "In this module we introduce Logistic Regression which can be regarded as the **simplest neural network**, a single \"neuron\". This type of network is sometime called a Perceptron, but the method used for learning is different from the way a Perceptron learns. \n",
        "\n",
        "As can be seen from the figure below the input is a vector of size _n_ and it feeds a single unit (a neuron or perceptron). To obtain the output we perform the **dot** product between the matrix **W** and the input **x** and the result is fed into some function (usually nonlinear) _f_\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "z&=\\sum_iw_i\\cdot x_i+b\\\\\n",
        "\\hat{y}(x)&=f(z)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Since $z$ depends on $w$ and $b$ so does $\\hat{y}$. The input and _f_ are known whereas _W_ and _b_ are parameters to be determined. Our goal is to find the _optimal_ _W_ and _b_ such that the output is as *close as possible* to the label associated with the input.\n",
        "![title](https://github.com/hikmatfarhat-ndu/CSC645/blob/master/figures/perceptron.png?raw=1)\n",
        "\n",
        "How is **as close as possible** defined? The dataset is usually a set of pairs $(x,y)$. We define the loss as the **deviation** between the lable $y$ and the result $\\hat{y}=f(z)$\n",
        "\n",
        "$$loss=E_{w,b}(y,\\hat{y})$$\n",
        "\n",
        "The function $E$ depends on the problem (for example binary cross entropy, mean squared error,...)\n",
        "\n",
        "Note that $E$ depends on the parameters $w,b$. Our goal is to find the **optimal** $w,b$ such that the loss is minimal. From calculus we know that to find the minimum (max) of a function we compute its derivative and find where it is null."
      ],
      "metadata": {
        "id": "4udcJFDWf8-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/hikmatfarhat-ndu/CSC645/blob/master/figures/gradient-descent.png?raw=1\" width=\"350\">\n",
        "</center>\n",
        "\n",
        "Now that we have an expression to optimize we need a method to find the optimal parameters. Typically, one computes the gradient and the optimal value corresponds to the value  of the parameters when the gradient vanishes. Unfortunately, for logistic regression there is __no closed form solution__ so we seek a numerical method to find the optimal parameters.\n",
        "\n",
        "Our goal is to find the **optimal** values for _W_ and _b_. To do so we give them some _arbitrary_ values and then using the expression for $E$\n",
        "In the figure below we show an arbitrary function _E(w)_. For a given value of _w_ we compute the derivative (slope) of _E_ with respect to _w_ (two different values are shown). The point on the left side has a negative slope so we need to **increase** the value of _w_ to move toward the minimum whereas the point on the right side the slope is positive so we have to **decrease** the value of _w_. \n",
        "\n",
        "In general we \"update\" the values of _w_ and _b_ as follows\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "  w=w-\\alpha\\cdot \\frac{\\partial E}{\\partial w}\\\\\n",
        "  b=b-\\alpha\\cdot \\frac{\\partial E}{\\partial b}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is a parameter chosen by us, called the __learning rate__."
      ],
      "metadata": {
        "id": "08C4C1wUcyQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatening the images\n",
        "The images have dimensions (3,32,32) (3 channels, 32 height,32 width). To feed them to our \"neuron\" we need to create a vector of dimension 3x32x32"
      ],
      "metadata": {
        "id": "_vMc-jV9pIcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dim=3*32*32\n",
        "train_samples=50000\n",
        "test_samples=10000\n",
        "img_train=img_train.reshape(train_samples,dim)\n",
        "img_test=img_test.reshape(test_samples,dim)"
      ],
      "metadata": {
        "id": "UvukYwIicPqx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All \"machines\" are given label 1 and living things label 0"
      ],
      "metadata": {
        "id": "E7QpEeoqppWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#airplane=0,car=1,bird=2,cat=3,deer=4,dog=5,frog=6,horse=7,ship=8,truck=9\n",
        "features=torch.tensor([0,1,8,9])\n",
        "for i in range(label_train.shape[0]):\n",
        "    if torch.isin(label_train[i],features):\n",
        "        label_train[i]=1\n",
        "    else:\n",
        "        label_train[i]=0\n",
        "\n",
        "for i in range(label_test.shape[0]):\n",
        "    if torch.isin(label_test[i],features):\n",
        "        label_test[i]=1\n",
        "    else:\n",
        "        label_test[i]=0        "
      ],
      "metadata": {
        "id": "WuNp40w4vebE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset is a bit biased\n",
        "torch.count_nonzero(label_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1V2d5mOt5FK",
        "outputId": "2ee3ff9b-25d4-439c-920a-d68738bf24b6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(20000)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the parameters"
      ],
      "metadata": {
        "id": "r-VxiVGnp2LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples=img_train.size()[1]\n",
        "dim=3*32*32\n",
        "weights=torch.rand(dim,requires_grad=True,dtype=torch.float32)\n",
        "weights.data/=train_samples\n",
        "bias=torch.tensor(0.,requires_grad=True,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "koYSysX4gKjr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rate=0.015\n",
        "##model=Net().cuda()\n",
        "import torch.optim as optim\n",
        "#optimizer=optim.SGD(model.parameters(),lr=0.015)\n",
        "loss_fn=torch.nn.BCELoss()\n",
        "\n",
        "for i in range(1000):\n",
        "  #optimizer.zero_grad()\n",
        "  #y_hat=model(img_train)\n",
        "  y_hat=torch.matmul(img_train,weights)+bias\n",
        "  y_hat=torch.sigmoid(y_hat)\n",
        "  loss=loss_fn(y_hat.squeeze(),label_train)\n",
        " # loss.backward()\n",
        "  dw,db=torch.autograd.grad(loss,[weights,bias])\n",
        " #optimizer.step()\n",
        "  \n",
        "  if(i%100==0):\n",
        "    print(loss)\n",
        "  weights.data-=rate*dw\n",
        "  bias.data-=rate*db\n",
        "  #weights.data-=rate*weights.grad.data\n",
        "  #bias.data-=rate*bias.grad.data\n",
        "  #bias.grad.data.zero_()\n",
        "  #weights.grad.data.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwNpWU0RoMbi",
        "outputId": "9d7bdbec-e951-4d18-8215-1a5ae35095a3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6942, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4717, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4585, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4523, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4480, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4448, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4423, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4401, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4384, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor(0.4368, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X):\n",
        "    \n",
        "    m = X.shape[0]\n",
        "    print(m)\n",
        "    #Y_prediction = torch.zeros(m,1).cuda()\n",
        "    Y_prediction = torch.zeros(m,1)\n",
        "    \n",
        "    # Compute vector \"Y_hat\" predicting\n",
        "    #    the probabilities of a machine being present in the picture\n",
        "    #p_hat= model(X)   # compute activation\n",
        "    y_hat=torch.matmul(X,weights)+bias\n",
        "    y_hat=torch.sigmoid(y_hat)\n",
        "    #print(p_hat.size())\n",
        "    #for i in range(p_hat.shape[0]):\n",
        "    print(y_hat.size())\n",
        "    for i in range(y_hat.shape[0]):    \n",
        "        # Convert probabilities Y_hat[0,i] to actual predictions p[0,i]\n",
        "        #if p_hat[i]>=0.5:\n",
        "        if y_hat[i]>=0.5:\n",
        "            Y_prediction[i]=1\n",
        "        else:\n",
        "            Y_prediction[i]=0\n",
        "    \n",
        "\n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "XttSuF7xuVom"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_test.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vXHYjobxI88",
        "outputId": "6df5b7af-207f-47c8-8af1-e04307d9172e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=predict(img_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6z-WCewxVJ8",
        "outputId": "ef29413e-f6f6-4209-a183-1b81a59d2e8f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "torch.Size([10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prediction_test = predict(img_test).squeeze()\n",
        "Y_prediction_train = predict(img_train).squeeze()\n",
        "#print(\"train accuracy:\"+str((100 - torch.mean(torch.abs(Y_prediction_train - label_train)) * 100)))\n",
        "print(\"test accuracy:\"+str((100 - torch.mean(torch.abs(Y_prediction_test - label_test)) * 100)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LDDffgKuv5P",
        "outputId": "a4098bc1-21b6-4b18-9cc6-2f1067c6bce3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "torch.Size([10000])\n",
            "50000\n",
            "torch.Size([50000])\n",
            "test accuracy:tensor(81.1900)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6GYmhnH9nR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "  #bias is True by default. It is included here for illustration\n",
        "    self.input_size=img_train.size()[1]\n",
        "    self.output_size=1\n",
        "    self.layer=nn.Linear(self.input_size,self.output_size,bias=True)\n",
        "  def forward(self,x):  \n",
        "    y_hat=self.layer(x)\n",
        "    y_hat=torch.sigmoid(y_hat)\n",
        "#   y_hat=torch.matmul(x,w)+b\n",
        "#   y_hat=torch.sigmoid(y_hat)\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "5LlED-xrgbiJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img_train=img_train.cuda()\n",
        "#img_test=img_test.cuda()\n",
        "#label_train=label_train.cuda()\n",
        "#label_test=label_test.cuda()"
      ],
      "metadata": {
        "id": "XEg5zCsUBYXt"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}