{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIIvHqFrnNFmIFc7a+CrFq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/pytorch/blob/main/custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estimating house prices\n",
        "So far we have dealt with classification problems, i.e. identifying to which category a given object belongs to. This notebook is the first to deal with regression problems. Here the output is a value(s). In particular, we will build a model to estimate a house price based on the zipcode, number of rooms, size, images..."
      ],
      "metadata": {
        "id": "nQAJNfiuAF3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What you will learn\n",
        " 1. Learng to use the pandas package\n",
        " 1. Building a custom dataset for Pytorch\n",
        " 1. Handling categorical data using one-hot encoding\n",
        " 1. Building a model that takes multimodal data as input"
      ],
      "metadata": {
        "id": "mm86ToUF_ckA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF1cj4oHQy6F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision as vision\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is in a Github repository. Our first task is to \"clean\" it by removing all houses belonging to zipcodes that occur less than 20 times in the dataset. The number 20 is arbitrary but it seems a good choice.\n",
        "The non-image features of the houses are in a .csv file, \"HousesInfo.csv\"  without headers. Each house has 4 images: bathroom, bedroom,front,kitchen. To simplify matters we choose to use only the frontal image. The prefix of the image files is the index of house as it occurs in the \"HousesInfo.csv\" **starting from 1**"
      ],
      "metadata": {
        "id": "PHXkKS3XAwdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "metadata": {
        "id": "KEZLbxXLRhDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the .csv file into a pandas data frame. The parameters are self explanatory."
      ],
      "metadata": {
        "id": "We_fkTfQAFKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"Houses-dataset/Houses Dataset/HousesInfo.txt\",header=None,delim_whitespace=True,\n",
        "               names=[\"bedrooms\",\"bathrooms\",\"size\",\"zipcode\",\"price\"])"
      ],
      "metadata": {
        "id": "JB3jj_iWR9AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove all entries with zipcodes occuring less than 20 times"
      ],
      "metadata": {
        "id": "lPa19JSa2tlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanData(df):\n",
        "    # compute the number of entries per zipcode\n",
        "    zipcodes=df['zipcode'].value_counts().keys().tolist()\n",
        "    counts=df['zipcode'].value_counts().tolist()\n",
        "    #discard all zipcodes ocurring less than 20 times\n",
        "    for count,zipcode in zip(counts,zipcodes):\n",
        "      if count<20:\n",
        "        idx=df[df['zipcode']==zipcode].index\n",
        "        df.drop(idx,inplace=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "O_YDMNrDjKPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=cleanData(df)"
      ],
      "metadata": {
        "id": "JEmjuzSwjMJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later we will split the dataset into training and testing so it is important we randomize the dataset before we do that"
      ],
      "metadata": {
        "id": "GibSiYUaGhyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#randomize the dataframe\n",
        "ran_dataset=dataset.sample(len(dataset))"
      ],
      "metadata": {
        "id": "3A-UNEYyJbDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train/test split and save to .csv files. The \"index\" is there to make sure we pick the correct image for each entry"
      ],
      "metadata": {
        "id": "btooq3pXG7I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset has 384 entries. Choose 310 for training and 74 for testing\n",
        "train_dataset=ran_dataset[0:310]\n",
        "test_dataset=ran_dataset[310:len(dataset)]\n",
        "train_dataset.to_csv(\"train.csv\",index_label=\"index\")\n",
        "test_dataset.to_csv(\"test.csv\",index_label=\"index\")"
      ],
      "metadata": {
        "id": "unzAxt0nHUGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Pytorch dataset\n",
        "\n",
        "To build a custom dataset we need to design a class that implements two methods\n",
        "1. \\_\\_len(self)\\__ should return the total number of items in the dataset \n",
        "1. \\_\\_getitem(self,index)\\_\\_ returns the item at \"index\"\n",
        "\n",
        "The above are the same methods needed to make an object [**iterable**](https://docs.python.org/3/glossary.html#term-iterable)\n",
        "\n",
        "**Note**: It is good practice to scale the inputs to small numbers by normalizing the data or dividing the values by the mean or max. If this is not done, the model might not converge, in many situations. "
      ],
      "metadata": {
        "id": "IXr9kmBZHGEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.io import read_image\n",
        "import os\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,csvFile,imgDir):\n",
        "    self.imgDir=imgDir\n",
        "    dataset=pd.read_csv(csvFile)\n",
        "    # convert the zipcode column into one-hot encoding\n",
        "    dummy=pd.get_dummies(dataset['zipcode'])\n",
        "    # the price and size column will be normalized\n",
        "    price=dataset['price']\n",
        "    size=dataset['size']\n",
        "    self.max_size=size.max()\n",
        "    self.max_price=price.max()\n",
        "    size=size/self.max_size\n",
        "    price=price/self.max_price\n",
        "    # remove the \"old\" columns of size,price and zipcode\n",
        "    # to prepar for the addition of the modified versions\n",
        "    df=dataset.drop(['size','price','zipcode'],axis=1)\n",
        "    self.data=pd.concat([df,size,dummy,price],axis=1)\n",
        "    self.resize=vision.transforms.Resize((48,48))\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    #\n",
        "    img_idx=self.data.iloc[idx,0]\n",
        "    # the images were labelled starting at 1. Pandas starts at 0\n",
        "    path=os.path.join(self.imgDir,str(img_idx+1)+\"_frontal.jpg\")\n",
        "    img=read_image(path)\n",
        "    img=self.resize(img)\n",
        "    return self.data.iloc[idx,1:-1].to_numpy(dtype=np.float32),img,np.float32(self.data.iloc[idx,-1])"
      ],
      "metadata": {
        "id": "Uuj8m0exRFgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=CustomDataset(\"train.csv\",\"Houses-dataset/Houses Dataset/\")\n",
        "test_dataset=CustomDataset(\"test.csv\",\"Houses-dataset/Houses Dataset/\")"
      ],
      "metadata": {
        "id": "d8sNvU2NCmEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=1,shuffle=False)"
      ],
      "metadata": {
        "id": "NvvvxTTe0I_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    self.relu=nn.ReLU() \n",
        "    self.fc1=nn.Linear(in_features=11,out_features=32)\n",
        "    self.fc2=nn.Linear(in_features=32,out_features=16)\n",
        "    self.fc3=nn.Linear(in_features=16,out_features=1)\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.fc2(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.fc3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "K6VOQQ1dHxZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD,Adam\n",
        "from torch.nn import MSELoss,L1Loss\n",
        "\n",
        "model=Net()\n",
        "optimizer=Adam(model.parameters())\n",
        "# one could use a mean squared error loss\n",
        "# but since our testing will be based on mean absolute error\n",
        "# we will use the corresponding loss\n",
        "#loss_fn=MSELoss()\n",
        "loss_fn=L1Loss()\n",
        "epochs=50\n",
        "for epoch in range(epochs):\n",
        "  for input,img,price in train_loader:\n",
        "    output=model(input)\n",
        "    loss=loss_fn(output.squeeze(),price)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(loss)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "lqu_RAVUM9t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that the datasets were \"normalized\" by dividing the values by the maximum value which is **different** for train and test data"
      ],
      "metadata": {
        "id": "BJaIcOw6kPTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "itr=iter(test_loader)\n",
        "input,img,price=next(itr)\n",
        "output=model(input)\n",
        "print(type(price))\n",
        "print(type(output))"
      ],
      "metadata": {
        "id": "za_z2n21nvP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total=0.0\n",
        "total2=0.0\n",
        "count=0\n",
        "max_test=test_dataset.max_price\n",
        "max_train=train_dataset.max_price\n",
        "for input,img,price in test_loader:\n",
        "  count+=1\n",
        "  output=model(input)\n",
        "  abs=torch.abs(output.item()*max_train-price*max_test)\n",
        "  print(np.abs(output.item()*max_train-price.item()*max_test))\n",
        "  total+=abs"
      ],
      "metadata": {
        "id": "_K0NHnUO-tej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total.mean()/count"
      ],
      "metadata": {
        "id": "fCnxPl41Mg8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AWGByezulZ00"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}